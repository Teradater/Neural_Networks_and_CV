{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08d63bec-f914-4a7a-a0a5-f9cdf479fcb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T11:30:33.126046Z",
     "iopub.status.busy": "2022-02-02T11:30:33.125795Z",
     "iopub.status.idle": "2022-02-02T11:30:33.139152Z",
     "shell.execute_reply": "2022-02-02T11:30:33.138583Z",
     "shell.execute_reply.started": "2022-02-02T11:30:33.126019Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Создаем входной массив из двух изображений RGB 3*3\n",
    "input_images = torch.tensor(\n",
    "      [[[[0,  1,  2],\n",
    "         [3,  4,  5],\n",
    "         [6,  7,  8]],\n",
    "\n",
    "        [[9, 10, 11],\n",
    "         [12, 13, 14],\n",
    "         [15, 16, 17]],\n",
    "\n",
    "        [[18, 19, 20],\n",
    "         [21, 22, 23],\n",
    "         [24, 25, 26]]],\n",
    "\n",
    "\n",
    "       [[[27, 28, 29],\n",
    "         [30, 31, 32],\n",
    "         [33, 34, 35]],\n",
    "\n",
    "        [[36, 37, 38],\n",
    "         [39, 40, 41],\n",
    "         [42, 43, 44]],\n",
    "\n",
    "        [[45, 46, 47],\n",
    "         [48, 49, 50],\n",
    "         [51, 52, 53]]]])\n",
    "\n",
    "\n",
    "def get_padding2d(input_images):\n",
    "    # padded_images = torch.zeros(input_images.shape[0], input_images.shape[1], input_images.shape[2]+2, input_images.shape[3]+2)\n",
    "    # for img_idx, img in enumerate(input_images):\n",
    "    #     for channel in range(img.shape[0]):\n",
    "    #         padded_images[img_idx][channel] = torch.nn.functional.pad(img[channel], pad=[1,1,1,1])\n",
    "            \n",
    "    # padded_images = torch.zeros([2, 3, 5, 5], dtype=torch.float32)\n",
    "    # padded_images[:, :, 1:-1, 1:-1] += input_images[:, :, :, :].type(torch.FloatTensor)\n",
    "    \n",
    "    padded_images = torch.nn.functional.pad(input_images, pad=(1,1,1,1)).float()\n",
    "    \n",
    "    return padded_images\n",
    "\n",
    "\n",
    "correct_padded_images = torch.tensor(\n",
    "       [[[[0.,  0.,  0.,  0.,  0.],\n",
    "          [0.,  0.,  1.,  2.,  0.],\n",
    "          [0.,  3.,  4.,  5.,  0.],\n",
    "          [0.,  6.,  7.,  8.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0.,  9., 10., 11.,  0.],\n",
    "          [0., 12., 13., 14.,  0.],\n",
    "          [0., 15., 16., 17.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 18., 19., 20.,  0.],\n",
    "          [0., 21., 22., 23.,  0.],\n",
    "          [0., 24., 25., 26.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]]],\n",
    "\n",
    "\n",
    "        [[[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 27., 28., 29.,  0.],\n",
    "          [0., 30., 31., 32.,  0.],\n",
    "          [0., 33., 34., 35.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 36., 37., 38.,  0.],\n",
    "          [0., 39., 40., 41.,  0.],\n",
    "          [0., 42., 43., 44.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 45., 46., 47.,  0.],\n",
    "          [0., 48., 49., 50.,  0.],\n",
    "          [0., 51., 52., 53.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]]]])\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "print(torch.allclose(get_padding2d(input_images), correct_padded_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "538344cd-8c1b-40dc-a907-4448612dc038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T11:46:31.191326Z",
     "iopub.status.busy": "2022-02-02T11:46:31.191071Z",
     "iopub.status.idle": "2022-02-02T11:46:31.196722Z",
     "shell.execute_reply": "2022-02-02T11:46:31.196138Z",
     "shell.execute_reply.started": "2022-02-02T11:46:31.191302Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
    "    out_shape = (\n",
    "        input_matrix_shape[0],\n",
    "        out_channels,\n",
    "        (input_matrix_shape[2] + padding * 2 - kernel_size) // stride + 1,\n",
    "        (input_matrix_shape[3] + padding * 2 - kernel_size) // stride + 1\n",
    "    )\n",
    "    return out_shape\n",
    "\n",
    "print(np.array_equal(\n",
    "    calc_out_shape(input_matrix_shape=[2, 3, 10, 10],\n",
    "                   out_channels=10,\n",
    "                   kernel_size=3,\n",
    "                   stride=1,\n",
    "                   padding=0),\n",
    "    [2, 10, 8, 8]))\n",
    "\n",
    "# ... и ещё несколько подобных кейсов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8442338-3291-41af-b370-9667a0e7f458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff0415c9-d38f-4c64-8851-1d2671d68180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T13:05:10.734822Z",
     "iopub.status.busy": "2022-02-02T13:05:10.734336Z",
     "iopub.status.idle": "2022-02-02T13:05:10.740923Z",
     "shell.execute_reply": "2022-02-02T13:05:10.739785Z",
     "shell.execute_reply.started": "2022-02-02T13:05:10.734757Z"
    }
   },
   "source": [
    "# В следующих примерах реализуем свертку сами\n",
    "**Обратите внимание, что везде отсутсвует bias!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c03f694c-c8f3-4a69-98af-0bd371a640bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T11:51:15.202251Z",
     "iopub.status.busy": "2022-02-02T11:51:15.201973Z",
     "iopub.status.idle": "2022-02-02T11:51:15.225311Z",
     "shell.execute_reply": "2022-02-02T11:51:15.224677Z",
     "shell.execute_reply.started": "2022-02-02T11:51:15.202226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вызвана функция test_conv2d_layer ей передан на тестирование класс  <class '__main__.Conv2d'>\n",
      "Kernel с помощю которого будет выполняться сворачивание \n",
      " tensor([[[[ 0.,  1.,  0.],\n",
      "          [ 1.,  2.,  1.],\n",
      "          [ 0.,  1.,  0.]],\n",
      "\n",
      "         [[ 1.,  2.,  1.],\n",
      "          [ 0.,  3.,  3.],\n",
      "          [ 0.,  1., 10.]],\n",
      "\n",
      "         [[10., 11., 12.],\n",
      "          [13., 14., 15.],\n",
      "          [16., 17., 18.]]]])\n",
      "Матрица, которую надо будет свернуть \n",
      " tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]],\n",
      "\n",
      "         [[16., 17., 18., 19.],\n",
      "          [20., 21., 22., 23.],\n",
      "          [24., 25., 26., 27.],\n",
      "          [28., 29., 30., 31.]],\n",
      "\n",
      "         [[32., 33., 34., 35.],\n",
      "          [36., 37., 38., 39.],\n",
      "          [40., 41., 42., 43.],\n",
      "          [44., 45., 46., 47.]]],\n",
      "\n",
      "\n",
      "        [[[48., 49., 50., 51.],\n",
      "          [52., 53., 54., 55.],\n",
      "          [56., 57., 58., 59.],\n",
      "          [60., 61., 62., 63.]],\n",
      "\n",
      "         [[64., 65., 66., 67.],\n",
      "          [68., 69., 70., 71.],\n",
      "          [72., 73., 74., 75.],\n",
      "          [76., 77., 78., 79.]],\n",
      "\n",
      "         [[80., 81., 82., 83.],\n",
      "          [84., 85., 86., 87.],\n",
      "          [88., 89., 90., 91.],\n",
      "          [92., 93., 94., 95.]]]])\n",
      "Класс  <class '__main__.Conv2d'>  выдает свернутую матрицу \n",
      " tensor([[[[ 5252.]]],\n",
      "\n",
      "\n",
      "        [[[12596.]]]], grad_fn=<MkldnnConvolutionBackward0>)\n",
      "Класс  <class '__main__.Conv2d'>  выдает свернутую матрицу \n",
      " tensor([[[[ 5252.]]],\n",
      "\n",
      "\n",
      "        [[[12596.]]]], grad_fn=<MkldnnConvolutionBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "# абстрактный класс для сверточного слоя\n",
    "## Абстрактный слой в данном случае нужен для того, чтобы не перезадавать переменные\n",
    "## Если мы делаем 2 разных класса, которые делают что-то похожее,\n",
    "##  то общую часть можно записать здесь и при создании нового класса наследовать этот\n",
    "class ABCConv2d(ABC):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.kernel = kernel\n",
    "## ниже указан декоратор @abstractmethod. Это значит, что при создании \n",
    "## нового класса на основании этого функцию нужно будет переопределить\n",
    "## иначе будет ошибка\n",
    "    @abstractmethod\n",
    "    def __call__(self, input_tensor):\n",
    "        pass\n",
    "\n",
    "\n",
    "# класс-обертка над torch.nn.Conv2d для унификации интерфейса\n",
    "## Это класс который выполняет свёртку двумерного слоя\n",
    "## согласно документации https://pytorch.org/docs/stable/nn.html#conv2d\n",
    "## выполняется это следующим образом. Сам conv2d это класс. \n",
    "## Для начала нужно передать ему параметры. После этого вызвать как функцию \n",
    "## с матрицей, которую нужно свернуть. \n",
    "class Conv2d(ABCConv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                      stride, padding=0, bias=False)\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.conv2d.weight.data = kernel\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        return self.conv2d(input_tensor)\n",
    "     \n",
    "\n",
    "\n",
    "# функция, создающая объект класса cls и возвращающая свертку от input_matrix\n",
    "## Эта функия подготоавливает данные и вызывает нужный класс для свертки\n",
    "## Сделано это для упрощения. \n",
    "## Обратите внимание самым первым параметром указывается\n",
    "## класс, которым нужно свернуть матрицу   conv2d_layer_class\n",
    "\n",
    "def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n",
    "    out_channels = kernel.shape[0]\n",
    "    in_channels = kernel.shape[1]\n",
    "    kernel_size = kernel.shape[2]\n",
    "\n",
    "    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n",
    "    layer.set_kernel(kernel)\n",
    "\n",
    "    return layer(input_matrix)\n",
    "\n",
    "\n",
    "# Функция, тестирующая класс conv2d_cls.\n",
    "## Возвращает True, если свертка совпадает со сверткой с помощью torch.nn.Conv2d.\n",
    "## Функция тестирует conv2d_layer_class. \n",
    "\n",
    "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
    "                      input_height=4, input_width=4, stride=2):\n",
    "    print('Вызвана функция test_conv2d_layer ей передан на тестирование класс ',conv2d_layer_class)\n",
    "    \n",
    "    kernel = torch.tensor(\n",
    "                      [[[[0., 1, 0],\n",
    "                         [1,  2, 1],\n",
    "                         [0,  1, 0]],\n",
    "\n",
    "                        [[1, 2, 1],\n",
    "                         [0, 3, 3],\n",
    "                         [0, 1, 10]],\n",
    "\n",
    "                        [[10, 11, 12],\n",
    "                         [13, 14, 15],\n",
    "                         [16, 17, 18]]]])\n",
    "    print('Kernel с помощю которого будет выполняться сворачивание \\n', kernel)\n",
    "    in_channels = kernel.shape[1]\n",
    "\n",
    "## создается тензор размерности 6 таблиц 4*4 \n",
    "## (то есть высота- 4, ширина -4) количество каналов - 3 (берется из размера kernel)\n",
    "## и 2 батча 2*3=6\n",
    "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
    "                                input_height * input_width,\n",
    "                                out=torch.FloatTensor()).reshape(batch_size, in_channels, input_height, input_width)\n",
    "    print('Матрица, которую надо будет свернуть \\n', input_tensor)\n",
    " ## Здесь вызываются 2 фукнции, которые выполняют сворачивание\n",
    " ## Одной передается класс conv2d_layer_class другой Conv2d\n",
    " ## Обратите внимание что conv2d_layer_class это аргумент текущей функции \n",
    " ## Эту функцию вызвали через print(test_conv2d_layer(Conv2d))\n",
    " ## то есть по факту сравнивается Conv2d и Conv2d\n",
    " \n",
    "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
    "        conv2d_layer_class, stride, kernel, input_tensor)\n",
    "    print('Класс ',conv2d_layer_class,' выдает свернутую матрицу \\n',custom_conv2d_out)\n",
    "\n",
    "    conv2d_out = create_and_call_conv2d_layer(\n",
    "        Conv2d, stride, kernel, input_tensor)\n",
    "    print('Класс ',Conv2d,' выдает свернутую матрицу \\n',conv2d_out)\n",
    "\n",
    "    return torch.allclose(custom_conv2d_out, conv2d_out) and (custom_conv2d_out.shape == conv2d_out.shape)\n",
    "\n",
    "print(test_conv2d_layer(Conv2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bd272-6d11-4265-9296-52837f557fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8abe9f1b-7279-49a1-a505-ca35b71eceda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T12:45:47.926713Z",
     "iopub.status.busy": "2022-02-02T12:45:47.926292Z",
     "iopub.status.idle": "2022-02-02T12:45:47.930674Z",
     "shell.execute_reply": "2022-02-02T12:45:47.929759Z",
     "shell.execute_reply.started": "2022-02-02T12:45:47.926656Z"
    }
   },
   "source": [
    "# Делаем конв слой циклами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2bbf343c-27c3-4737-a141-d038a3f9c9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T12:32:04.417431Z",
     "iopub.status.busy": "2022-02-02T12:32:04.416994Z",
     "iopub.status.idle": "2022-02-02T12:32:04.458847Z",
     "shell.execute_reply": "2022-02-02T12:32:04.458133Z",
     "shell.execute_reply.started": "2022-02-02T12:32:04.417375Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
    "    batch_size, channels_count, input_height, input_width = input_matrix_shape\n",
    "    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "    return batch_size, out_channels, output_height, output_width\n",
    "\n",
    "\n",
    "class ABCConv2d(ABC):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.kernel = kernel\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, input_tensor):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Conv2d(ABCConv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                      stride, padding=0, bias=False)\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.conv2d.weight.data = kernel\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        return self.conv2d(input_tensor)\n",
    "\n",
    "\n",
    "def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n",
    "    out_channels = kernel.shape[0]\n",
    "    in_channels = kernel.shape[1]\n",
    "    kernel_size = kernel.shape[2]\n",
    "\n",
    "    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n",
    "    layer.set_kernel(kernel)\n",
    "\n",
    "    return layer(input_matrix)\n",
    "\n",
    "\n",
    "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
    "                      input_height=4, input_width=4, stride=2):\n",
    "    kernel = torch.tensor(\n",
    "                      [[[[0., 1, 0],\n",
    "                         [1,  2, 1],\n",
    "                         [0,  1, 0]],\n",
    "\n",
    "                        [[1, 2, 1],\n",
    "                         [0, 3, 3],\n",
    "                         [0, 1, 10]],\n",
    "\n",
    "                        [[10, 11, 12],\n",
    "                         [13, 14, 15],\n",
    "                         [16, 17, 18]]]])\n",
    "\n",
    "    in_channels = kernel.shape[1]\n",
    "\n",
    "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
    "                                input_height * input_width,\n",
    "                                out=torch.FloatTensor()) \\\n",
    "        .reshape(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
    "        conv2d_layer_class, stride, kernel, input_tensor)\n",
    "    \n",
    "    conv2d_out = create_and_call_conv2d_layer(\n",
    "        Conv2d, stride, kernel, input_tensor)\n",
    "\n",
    "    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n",
    "             and (custom_conv2d_out.shape == conv2d_out.shape)\n",
    "\n",
    "\n",
    "# Сверточный слой через циклы.\n",
    "class Conv2dLoop(ABCConv2d):\n",
    "    def __call__(self, input_tensor):\n",
    "        \n",
    "        batch_size, out_channels, output_height, output_width = calc_out_shape(\n",
    "                                input_tensor.shape, \n",
    "                                self.out_channels,\n",
    "                                self.kernel_size,\n",
    "                                self.stride,\n",
    "                                padding=0)\n",
    "            \n",
    "        # создадим выходной тензор, заполненный нулями         \n",
    "        output_tensor = torch.zeros(batch_size, out_channels, output_height, output_width)\n",
    "        \n",
    "        # вычисление свертки с использованием циклов.\n",
    "        # цикл по входным батчам(изображениям)\n",
    "        for num_batch, batch in enumerate(input_tensor): \n",
    "             \n",
    "            # цикл по фильтрам (количество фильтров совпадает с количеством выходных каналов)  \n",
    "            for num_kernel, kernel in enumerate(self.kernel):\n",
    "            \n",
    "                # цикл по размерам выходного изображения\n",
    "                for i in range(output_height):\n",
    "                    for j in range(output_width): \n",
    "                        \n",
    "                        # вырезаем кусочек из батча (сразу по всем входным каналам)\n",
    "                        current_row = self.stride*i\n",
    "                        current_column = self.stride*j\n",
    "                        current_slice = batch[:, current_row:current_row + self.kernel_size, current_column:current_column + self.kernel_size]\n",
    "                        \n",
    "                        # умножаем кусочек на фильтр\n",
    "                        res = float((current_slice * kernel).sum())\n",
    "                        \n",
    "                        # заполняем ячейку в выходном тензоре\n",
    "                        output_tensor[num_batch,num_kernel,i,j] = res\n",
    "                        \n",
    "        return output_tensor\n",
    "\n",
    "# Корректность реализации определится в сравнении со стандартным слоем из pytorch.\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "print(test_conv2d_layer(Conv2dLoop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ef550-2a0e-47f9-b7a2-6dd9cb4b6f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa7328e8-906d-4b92-bdcd-0af77529ca4e",
   "metadata": {},
   "source": [
    "# Избавляемся от циклов, вводим матричные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a3a8513-a1f1-49ee-8e19-da1785c52c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T13:00:15.632589Z",
     "iopub.status.busy": "2022-02-02T13:00:15.632113Z",
     "iopub.status.idle": "2022-02-02T13:00:15.659280Z",
     "shell.execute_reply": "2022-02-02T13:00:15.658479Z",
     "shell.execute_reply.started": "2022-02-02T13:00:15.632537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class Conv2dMatrix(ABCConv2d):\n",
    "    # Функция преобразование кернела в матрицу нужного вида.\n",
    "    \n",
    "    # Вариант для любой размерности кернелов, числа входных и выходных каналов, размеров изображений. Присутствует два вложенных цикла. \n",
    "    # def _unsqueeze_kernel(self, torch_input, output_height, output_width):\n",
    "    #     _, in_channels, in_height, in_width = torch_input.shape\n",
    "    #     ku_size = [self.out_channels, output_height, output_width, in_channels, in_height, in_width]\n",
    "    #     kernel_unsqueezed = torch.zeros(ku_size, dtype=torch.float32)\n",
    "    #     for i in range(output_height):\n",
    "    #         for j in range(output_width):\n",
    "    #             h_slice = slice(i*self.stride, i*self.stride+self.kernel_size)\n",
    "    #             w_slice = slice(j*self.stride, j*self.stride+self.kernel_size)\n",
    "    #             kernel_unsqueezed[:, i, j, :, h_slice, w_slice] = self.kernel.type(torch.float32)\n",
    "    #     return kernel_unsqueezed.view(-1, in_channels*in_height*in_width)\n",
    "    \n",
    "    # без циклов, для всяких размерностей. тренировка torch-а и матричных махинаций\n",
    "    def _unsqueeze_kernel(self, torch_input, output_height, output_width):\n",
    "        img_size_rows = torch_input.shape[2]\n",
    "        img_size_cols = torch_input.shape[3]\n",
    "        as_oneline_filters = (self.out_channels, 1, -1)\n",
    "\n",
    "        # шаблон - \"одно применение\" ядра\n",
    "        m = torch.nn.ZeroPad2d((0, img_size_cols - self.kernel_size, \n",
    "                                0, img_size_rows - self.kernel_size))\n",
    "        kernel_unsqueezed = m(self.kernel).view(as_oneline_filters)\n",
    "\n",
    "        # \"применение\" ядра к строке изображения\n",
    "        # добавляем шаблону \"self.stride\" нулей \n",
    "        m = torch.nn.ConstantPad1d((0, self.stride), 0)\n",
    "        kernel_unsqueezed = m(kernel_unsqueezed)\n",
    "        # и создаем \"output_width\" копий для каждого фильтра\n",
    "        copier = torch.ones(self.out_channels, output_width, 1)\n",
    "        kernel_unsqueezed = (copier @ kernel_unsqueezed)\n",
    "        # вытягиваем каждый фильтир в строку и обрезаем хвост\n",
    "        m = torch.nn.ConstantPad1d((0, - self.stride * output_width), 0)\n",
    "        kernel_unsqueezed = m(kernel_unsqueezed.view(as_oneline_filters))\n",
    "        \n",
    "        # аналогично для \"применения\" ядра по строкам изображения\n",
    "        # копируем полученный шаблон \"output_width\" раз со сдвигом\n",
    "        m = torch.nn.ConstantPad1d((0, self.stride * img_size_cols), 0)\n",
    "        kernel_unsqueezed = m(kernel_unsqueezed)\n",
    "        # создаем \"output_height\" копий для каждого фильтра\n",
    "        copier = torch.ones(self.out_channels, output_height, 1)\n",
    "        kernel_unsqueezed = (copier @ kernel_unsqueezed)\n",
    "        # вытягиваем каждый фильтир в строку и обрезаем хвост\n",
    "        m = torch.nn.ConstantPad1d((0, - self.stride * output_height * img_size_cols), 0)\n",
    "        kernel_unsqueezed = m(kernel_unsqueezed.view(as_oneline_filters))\n",
    "\n",
    "        # вытянутым в строку фильтрам придаем требуемый размер\n",
    "        return kernel_unsqueezed.view((self.out_channels * output_height * output_width, -1))\n",
    "\n",
    "    def __call__(self, torch_input):\n",
    "        batch_size, out_channels, output_height, output_width\\\n",
    "            = calc_out_shape(\n",
    "                input_matrix_shape=torch_input.shape,\n",
    "                out_channels=self.kernel.shape[0],\n",
    "                kernel_size=self.kernel.shape[2],\n",
    "                stride=self.stride,\n",
    "                padding=0)\n",
    "\n",
    "        kernel_unsqueezed = self._unsqueeze_kernel(torch_input, output_height, output_width)\n",
    "        result = kernel_unsqueezed @ torch_input.view((batch_size, -1)).permute(1, 0)\n",
    "        return result.permute(1, 0).view((batch_size, self.out_channels,\n",
    "                                          output_height, output_width))\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "print(test_conv2d_layer(Conv2dMatrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c6342-5ab5-484e-8461-abd52454a6e6",
   "metadata": {},
   "source": [
    "# Ускоряем еще лучше - избавляемся от кучи нулей в матрицах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "582ddf73-250f-480b-9c78-9e151fab9078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T13:01:12.496648Z",
     "iopub.status.busy": "2022-02-02T13:01:12.496309Z",
     "iopub.status.idle": "2022-02-02T13:01:12.512578Z",
     "shell.execute_reply": "2022-02-02T13:01:12.511860Z",
     "shell.execute_reply.started": "2022-02-02T13:01:12.496602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class Conv2dMatrixV2(ABCConv2d):\n",
    "    # Функция преобразования кернела в нужный формат.\n",
    "    def _convert_kernel(self):\n",
    "        converted_kernel = self.kernel.flatten(start_dim=1)\n",
    "        return converted_kernel\n",
    "\n",
    "    # Функция преобразования входа в нужный формат.\n",
    "    def _convert_input(self, torch_input, output_height, output_width):\n",
    "        converted_input = torch.zeros([torch_input.size(0), \n",
    "                                       self.in_channels * self.kernel_size**2, \n",
    "                                       output_width * output_height])\n",
    "        ks = self.kernel_size\n",
    "        sd = self.stride\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                res = torch_input[:, :, h*sd:h*sd+ks, w*sd:w*sd+ks].flatten(start_dim=1)\n",
    "                converted_input[:, :, h * output_width + w] = res\n",
    "        return converted_input\n",
    "\n",
    "    def __call__(self, torch_input):\n",
    "        batch_size, out_channels, output_height, output_width\\\n",
    "            = calc_out_shape(\n",
    "                input_matrix_shape=torch_input.shape,\n",
    "                out_channels=self.kernel.shape[0],\n",
    "                kernel_size=self.kernel.shape[2],\n",
    "                stride=self.stride,\n",
    "                padding=0)\n",
    "\n",
    "        converted_kernel = self._convert_kernel()\n",
    "        converted_input = self._convert_input(torch_input, output_height, output_width)\n",
    "\n",
    "        conv2d_out_alternative_matrix_v2 = converted_kernel @ converted_input\n",
    "        return conv2d_out_alternative_matrix_v2.view(torch_input.shape[0],\n",
    "                                                     self.out_channels, output_height,\n",
    "                                                     output_width)\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "print(test_conv2d_layer(Conv2dMatrixV2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271c278-a61f-48c8-a346-b4295ef65bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learn_v2",
   "language": "python",
   "name": "rl_learn_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
